Technitium DNS Server has been deployed!

1. DNS Web Console:
{{- if .Values.ingress.enabled }}
  Access the web console at:
  {{- range $host := .Values.ingress.hosts }}
    http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}
  {{- end }}
{{- else if contains "NodePort" .Values.serviceWeb.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "technitium-dns.fullname" . }}-web)
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.serviceWeb.type }}
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
  Watch the status with:
    kubectl get svc --namespace {{ .Release.Namespace }} {{ include "technitium-dns.fullname" . }}-web -w
{{- else }}
  kubectl port-forward --namespace {{ .Release.Namespace }} svc/{{ include "technitium-dns.fullname" . }}-web {{ .Values.serviceWeb.httpPort }}:{{ .Values.serviceWeb.httpPort }}
  Then visit: http://localhost:{{ .Values.serviceWeb.httpPort }}
{{- end }}

2. DNS Service:
{{- if contains "LoadBalancer" .Values.serviceDns.type }}
  DNS is exposed via LoadBalancer. Get the IP with:
    kubectl get svc --namespace {{ .Release.Namespace }} {{ include "technitium-dns.fullname" . }}-dns -w
{{- else if contains "NodePort" .Values.serviceDns.type }}
  DNS is exposed via NodePort on all cluster nodes.
{{- else }}
  DNS is available internally at: {{ include "technitium-dns.fullname" . }}-dns.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.serviceDns.port }}
{{- end }}

3. Default credentials: admin / admin (change immediately!)
{{- if and .Values.admin.enabled (not .Values.admin.existingSecret) .Values.admin.password }}
   NOTE: An admin password was set via Helm values.
{{- end }}
{{- if and .Values.clustering.enabled (gt (int .Values.replicaCount) 1) }}

4. CLUSTERING SETUP REQUIRED:
  You have {{ .Values.replicaCount }} replicas with clustering enabled.
  After all pods are running, you must configure clustering via the web console.

  a) Access the PRIMARY node (pod-0) web console:
     kubectl port-forward --namespace {{ .Release.Namespace }} pod/{{ include "technitium-dns.fullname" . }}-0 {{ .Values.webConsole.httpPort }}:{{ .Values.webConsole.httpPort }}
     Then visit: http://localhost:{{ .Values.webConsole.httpPort }}
     (Or use the primary service: {{ include "technitium-dns.fullname" . }}-web-primary)

  b) On the primary node (pod-0), go to Settings > Clustering and initialize
     the cluster. This will create the cluster zone and TSIG key.

  c) Copy the TSIG key name, shared secret, and cluster domain from the primary.

  d) Access each SECONDARY node web console:
     {{- $fullname := include "technitium-dns.fullname" . }}
     {{- range $i := until (int .Values.replicaCount) }}
     {{- if ne $i 0 }}
     kubectl port-forward --namespace {{ $.Release.Namespace }} pod/{{ $fullname }}-{{ $i }} {{ $.Values.webConsole.httpPort }}:{{ $.Values.webConsole.httpPort }}
     {{- end }}
     {{- end }}

  e) On each secondary, go to Settings > Clustering and join the cluster
     using the TSIG key and cluster domain from step (c).
     Set the primary address to:
       {{ include "technitium-dns.fullname" . }}-0.{{ include "technitium-dns.fullname" . }}-headless.{{ .Release.Namespace }}.svc.cluster.local

  f) Get the node IPs for the cluster zone A records:
     {{- if .Values.clustering.hostNetwork }}

     HOST NETWORK MODE: Pods use the node's IP directly.
     Get each pod's node IP with:
     {{- range $i := until (int .Values.replicaCount) }}
     {{ $fullname }}-{{ $i }}: kubectl get pod --namespace {{ $.Release.Namespace }} {{ $fullname }}-{{ $i }} -o jsonpath='{.status.hostIP}'
     {{- end }}

     These IPs are STABLE as long as pods stay pinned to the same nodes.
     To ensure pods always schedule on the same node:
       1. Label {{ .Values.replicaCount }} nodes: kubectl label node <name> technitium-dns=true
       2. Set nodeSelector: { technitium-dns: "true" } in your values
       3. Use local-path storage (K3s default) - PV node affinity locks
          each pod to its original node after first scheduling
     {{- else }}
     kubectl get svc --namespace {{ .Release.Namespace }} -l app={{ include "technitium-dns.name" . }} | grep node
     {{- range $i := until (int .Values.replicaCount) }}
     {{ $fullname }}-node-{{ $i }}: kubectl get svc --namespace {{ $.Release.Namespace }} {{ $fullname }}-node-{{ $i }} -o jsonpath='{.spec.clusterIP}'
     {{- end }}

     These ClusterIPs are STATIC and survive pod restarts. Use them as the
     node IP addresses in the Technitium cluster zone A records.
     {{- end }}
{{- end }}
